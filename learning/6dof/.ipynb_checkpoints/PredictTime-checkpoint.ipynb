{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# note1:  NN best\n",
    "# note2:  XGBoost not working\n",
    "# note3:  KNN not working\n",
    "# note4:  KRR not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../../data/6dof/predictTime_X.npy\")\n",
    "Y = np.load(\"../../data/6dof/predictTime_Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParams(network:torch.nn.Module, decay:float) -> list:\n",
    "        ''' function to set weight decay\n",
    "        '''\n",
    "        params_dict = dict(network.named_parameters())\n",
    "        params=[]\n",
    "        weights=[]\n",
    "\n",
    "        for key, value in params_dict.items():\n",
    "            if key[-4:] == 'bias':\n",
    "                params += [{'params':value,'weight_decay':0.0}]\n",
    "            else:\n",
    "                params +=  [{'params': value,'weight_decay':decay}]\n",
    "        return params\n",
    "    \n",
    "class FlexDMPForcing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(12, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_num = 1000\n",
    "index_whole = np.arange(len(X))\n",
    "np.random.shuffle(index_whole)\n",
    "train_X, train_Y = X[index_whole[:-test_data_num]], Y[index_whole[:-test_data_num]]\n",
    "test_X, test_Y = X[index_whole[-test_data_num:]], Y[index_whole[-test_data_num:]]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "train_X, train_Y = torch.from_numpy(train_X).float().to(device), \\\n",
    "            torch.from_numpy(train_Y).float().to(device)\n",
    "test_X, test_Y = torch.from_numpy(test_X).float().to(device), \\\n",
    "            torch.from_numpy(test_Y).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FlexDMPForcing().to(device)\n",
    "param = setParams(net, decay=1e-2)\n",
    "optimizer = Adam(params=param, lr=4e-3)\n",
    "# optimizer = Adam(params=net.parameters(), lr=2e-3)\n",
    "batch_size = 100\n",
    "Loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  475.1540258778085 Vali:  76.70867919921875 MAE:  0.02687535\n",
      "Train:  58.355231234324066 Vali:  35.49694061279297 MAE:  0.018200025\n",
      "Train:  39.93211050239398 Vali:  48.503440856933594 MAE:  0.022690572\n",
      "Train:  34.91519029534978 Vali:  25.708641052246094 MAE:  0.015934104\n",
      "Train:  30.78631496909711 Vali:  22.100875854492188 MAE:  0.0141546605\n",
      "Train:  29.757930026980613 Vali:  26.049463272094727 MAE:  0.015321697\n",
      "Train:  28.36921476528799 Vali:  21.949079513549805 MAE:  0.014498789\n",
      "Train:  26.554250162797008 Vali:  20.457653045654297 MAE:  0.013568659\n",
      "Train:  25.471577642289855 Vali:  28.862993240356445 MAE:  0.016259653\n",
      "Train:  24.552807660411588 Vali:  21.925922393798828 MAE:  0.014184085\n",
      "Train:  24.1180843860983 Vali:  18.049671173095703 MAE:  0.012490767\n",
      "Train:  24.685105304580798 Vali:  21.143844604492188 MAE:  0.0136197675\n",
      "Train:  23.077564088560695 Vali:  19.618398666381836 MAE:  0.012859181\n",
      "Train:  23.025212578121707 Vali:  19.272289276123047 MAE:  0.01357787\n",
      "Train:  23.34838487433015 Vali:  19.486154556274414 MAE:  0.013532163\n",
      "Train:  22.208636696218587 Vali:  22.675371170043945 MAE:  0.0149612455\n",
      "Train:  22.169046017077328 Vali:  18.865135192871094 MAE:  0.012971938\n",
      "Train:  22.205644263809532 Vali:  18.772693634033203 MAE:  0.013415687\n",
      "Train:  21.57488990481809 Vali:  18.646146774291992 MAE:  0.01310749\n",
      "Train:  21.515124424584478 Vali:  19.66131019592285 MAE:  0.013560558\n",
      "Train:  21.4927234896653 Vali:  18.158117294311523 MAE:  0.013018853\n",
      "Train:  21.331031645630762 Vali:  33.76237487792969 MAE:  0.019199967\n",
      "Train:  21.42276465326762 Vali:  17.7940673828125 MAE:  0.012431019\n",
      "Train:  21.272093947842823 Vali:  19.68791961669922 MAE:  0.013688308\n",
      "Train:  21.44989173669609 Vali:  17.411563873291016 MAE:  0.012355061\n",
      "Train:  21.307373715133117 Vali:  19.082801818847656 MAE:  0.013394886\n",
      "Train:  20.47008049916878 Vali:  18.461828231811523 MAE:  0.013012353\n",
      "Train:  21.048413738415395 Vali:  17.585481643676758 MAE:  0.012326622\n",
      "Train:  20.516768776598592 Vali:  20.492115020751953 MAE:  0.014018034\n",
      "Train:  20.745683228883813 Vali:  17.02387237548828 MAE:  0.012237101\n",
      "Train:  20.301128384706786 Vali:  26.70133399963379 MAE:  0.016738193\n",
      "Train:  20.61877916253728 Vali:  20.858165740966797 MAE:  0.013494821\n",
      "Train:  20.08917214304423 Vali:  23.865989685058594 MAE:  0.014294566\n",
      "Train:  20.227236242088484 Vali:  18.963314056396484 MAE:  0.013171256\n",
      "Train:  20.249938424199605 Vali:  22.099428176879883 MAE:  0.013830597\n",
      "Train:  20.073172479396245 Vali:  19.52429962158203 MAE:  0.0136323\n",
      "Train:  20.030545808943057 Vali:  21.528348922729492 MAE:  0.014569109\n",
      "Train:  19.67934716725521 Vali:  19.00483512878418 MAE:  0.013580643\n",
      "Train:  20.009946907510002 Vali:  18.067481994628906 MAE:  0.012124288\n",
      "Train:  19.70546606393169 Vali:  18.013723373413086 MAE:  0.012584275\n",
      "Train:  19.586187385998183 Vali:  17.963645935058594 MAE:  0.012677528\n",
      "Train:  19.449493341651753 Vali:  19.818077087402344 MAE:  0.013875267\n",
      "Train:  19.506689713155623 Vali:  17.503236770629883 MAE:  0.012782355\n",
      "Train:  19.021568133676652 Vali:  18.559463500976562 MAE:  0.01226346\n",
      "Train:  18.988986084615583 Vali:  16.19188117980957 MAE:  0.011843982\n",
      "Train:  18.58933472804886 Vali:  18.448829650878906 MAE:  0.013061301\n",
      "Train:  18.818413064805725 Vali:  20.267473220825195 MAE:  0.014203856\n",
      "Train:  18.645117335353824 Vali:  18.195327758789062 MAE:  0.0128590595\n",
      "Train:  18.915735145266964 Vali:  16.548633575439453 MAE:  0.011696023\n",
      "Train:  18.639910968259084 Vali:  16.986583709716797 MAE:  0.012616033\n",
      "Train:  18.707139267681313 Vali:  15.97337818145752 MAE:  0.011820879\n",
      "Train:  18.379732013949386 Vali:  16.230371475219727 MAE:  0.011842915\n",
      "Train:  18.527097725353652 Vali:  18.356746673583984 MAE:  0.013126401\n",
      "Train:  18.26318002433228 Vali:  20.304580688476562 MAE:  0.0142726265\n",
      "Train:  18.20922739385701 Vali:  16.784862518310547 MAE:  0.011634427\n",
      "Train:  18.304035276646236 Vali:  18.415496826171875 MAE:  0.012631429\n",
      "Train:  18.220705425482002 Vali:  16.1433162689209 MAE:  0.0114114685\n",
      "Train:  18.354004915498145 Vali:  16.188743591308594 MAE:  0.011856368\n",
      "Train:  17.839687767989343 Vali:  17.99289321899414 MAE:  0.0123084765\n",
      "Train:  18.04423603359744 Vali:  17.905004501342773 MAE:  0.012437163\n",
      "Train:  18.077678926728613 Vali:  18.118783950805664 MAE:  0.0123902755\n",
      "Train:  17.905201839200025 Vali:  20.697059631347656 MAE:  0.014350239\n",
      "Train:  17.826228863558324 Vali:  17.3975772857666 MAE:  0.012230385\n",
      "Train:  17.665378991133874 Vali:  16.8083553314209 MAE:  0.012141675\n",
      "Train:  17.799582473837216 Vali:  20.365680694580078 MAE:  0.013405691\n",
      "Train:  17.817650733234213 Vali:  16.574628829956055 MAE:  0.011331977\n",
      "Train:  17.63605894287713 Vali:  16.187801361083984 MAE:  0.011450064\n",
      "Train:  17.510646328480124 Vali:  15.959609985351562 MAE:  0.011576923\n",
      "Train:  17.560780368777486 Vali:  16.968122482299805 MAE:  0.0121779675\n",
      "Train:  17.74033570666965 Vali:  16.927406311035156 MAE:  0.011907519\n",
      "Train:  17.508433115568092 Vali:  16.40604019165039 MAE:  0.011931944\n",
      "Train:  17.478316901227554 Vali:  18.89878273010254 MAE:  0.013424901\n",
      "Train:  17.389201443486932 Vali:  16.742961883544922 MAE:  0.012343146\n",
      "Train:  17.52096426950084 Vali:  14.830933570861816 MAE:  0.011098518\n",
      "Train:  17.61934638915302 Vali:  17.93950080871582 MAE:  0.012530667\n",
      "Train:  17.51435204581391 Vali:  16.03679656982422 MAE:  0.012214783\n",
      "Train:  17.47334350270333 Vali:  16.311168670654297 MAE:  0.011871189\n",
      "Train:  17.386290079569644 Vali:  15.439002990722656 MAE:  0.011217006\n",
      "Train:  17.28265979856038 Vali:  16.96684455871582 MAE:  0.011728868\n",
      "Train:  17.341124894121567 Vali:  14.564138412475586 MAE:  0.010796459\n",
      "Train:  17.217536581162925 Vali:  17.583168029785156 MAE:  0.013271561\n",
      "Train:  17.37536343910711 Vali:  15.704812049865723 MAE:  0.011278526\n",
      "Train:  16.933902449230494 Vali:  13.817076683044434 MAE:  0.011049373\n",
      "Train:  17.05234451980042 Vali:  16.80474281311035 MAE:  0.011850661\n",
      "Train:  17.092751559936744 Vali:  15.877371788024902 MAE:  0.012084808\n",
      "Train:  16.948706942153493 Vali:  15.134943962097168 MAE:  0.011293667\n",
      "Train:  16.941866726497953 Vali:  15.141168594360352 MAE:  0.011189993\n",
      "Train:  16.938162051344946 Vali:  17.719661712646484 MAE:  0.012715457\n",
      "Train:  16.978590523081717 Vali:  15.748845100402832 MAE:  0.011708911\n",
      "Train:  16.868381141415604 Vali:  15.154338836669922 MAE:  0.011265005\n",
      "Train:  16.884246599759987 Vali:  14.597530364990234 MAE:  0.011125896\n",
      "Train:  17.050857260587403 Vali:  16.020771026611328 MAE:  0.012195688\n",
      "Train:  16.87099529959315 Vali:  14.567440032958984 MAE:  0.010738509\n",
      "Train:  16.779585173147186 Vali:  18.12295150756836 MAE:  0.013239412\n",
      "Train:  16.578686911768195 Vali:  17.405040740966797 MAE:  0.01184752\n",
      "Train:  16.71669665549299 Vali:  17.843820571899414 MAE:  0.013314373\n",
      "Train:  16.907937152437167 Vali:  15.963338851928711 MAE:  0.012047879\n",
      "Train:  16.762655632444424 Vali:  16.82823944091797 MAE:  0.013005504\n",
      "Train:  16.721055839387635 Vali:  17.324207305908203 MAE:  0.012110934\n",
      "Train:  16.73810020282114 Vali:  18.414674758911133 MAE:  0.013293232\n",
      "Train:  16.760787036607592 Vali:  17.77886199951172 MAE:  0.012212733\n",
      "Train:  16.58747345423527 Vali:  15.892663955688477 MAE:  0.011961765\n",
      "Train:  16.431635781843884 Vali:  17.17873191833496 MAE:  0.011789659\n",
      "Train:  16.542395561890636 Vali:  16.98358154296875 MAE:  0.012751452\n",
      "Train:  16.48013912139179 Vali:  15.220745086669922 MAE:  0.011213994\n",
      "Train:  16.12514137741473 Vali:  14.450051307678223 MAE:  0.010842827\n",
      "Train:  16.124061621178825 Vali:  14.417692184448242 MAE:  0.010999333\n",
      "Train:  16.087086231931508 Vali:  21.242504119873047 MAE:  0.01485483\n",
      "Train:  16.108632571062596 Vali:  15.706019401550293 MAE:  0.011826443\n",
      "Train:  16.01386741356884 Vali:  14.441375732421875 MAE:  0.010794804\n",
      "Train:  15.884093470196072 Vali:  15.050508499145508 MAE:  0.011190222\n",
      "Train:  15.974122355824752 Vali:  14.575108528137207 MAE:  0.010826931\n",
      "Train:  16.102402848305463 Vali:  15.737378120422363 MAE:  0.01176919\n",
      "Train:  15.95312637219326 Vali:  14.902162551879883 MAE:  0.011299713\n",
      "Train:  15.970263215963788 Vali:  14.16840648651123 MAE:  0.011253694\n",
      "Train:  15.87630595474792 Vali:  14.02613639831543 MAE:  0.01116765\n",
      "Train:  16.048573250393215 Vali:  14.39648151397705 MAE:  0.01100241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  15.760264692032079 Vali:  13.94686222076416 MAE:  0.0104223825\n",
      "Train:  15.829565774108008 Vali:  14.750288009643555 MAE:  0.011376124\n",
      "Train:  15.807048472054571 Vali:  16.135009765625 MAE:  0.011244106\n",
      "Train:  15.867811364578687 Vali:  14.3790283203125 MAE:  0.011221546\n",
      "Train:  15.780317067413879 Vali:  14.3055419921875 MAE:  0.01085884\n",
      "Train:  15.848277854233338 Vali:  15.466344833374023 MAE:  0.011773771\n",
      "Train:  15.799984871226249 Vali:  17.351789474487305 MAE:  0.011782646\n",
      "Train:  15.84267937708244 Vali:  17.345966339111328 MAE:  0.011985672\n",
      "Train:  15.811988975332795 Vali:  14.900272369384766 MAE:  0.010843009\n",
      "Train:  15.805267118206984 Vali:  14.417287826538086 MAE:  0.010801116\n",
      "Train:  15.746326161117004 Vali:  14.535195350646973 MAE:  0.010768088\n",
      "Train:  15.92181092474958 Vali:  13.184428215026855 MAE:  0.010373789\n",
      "Train:  15.601316731953792 Vali:  18.0274600982666 MAE:  0.013404735\n",
      "Train:  15.762691141032487 Vali:  14.07356071472168 MAE:  0.010655615\n",
      "Train:  15.668299424905571 Vali:  14.207889556884766 MAE:  0.010942475\n",
      "Train:  15.608470831672065 Vali:  14.786493301391602 MAE:  0.010911118\n",
      "Train:  15.706478744273562 Vali:  14.051100730895996 MAE:  0.01070213\n",
      "Train:  15.722590819708735 Vali:  14.8759126663208 MAE:  0.011124925\n",
      "Train:  15.650836663623508 Vali:  14.586394309997559 MAE:  0.0109306425\n",
      "Train:  15.718432170195545 Vali:  19.006563186645508 MAE:  0.013822558\n",
      "Train:  15.69672859692745 Vali:  13.935260772705078 MAE:  0.010522636\n",
      "Train:  15.624358876660573 Vali:  14.501069068908691 MAE:  0.010802786\n",
      "Train:  15.680684822754895 Vali:  13.761959075927734 MAE:  0.011034071\n",
      "Train:  15.552528618394042 Vali:  14.909734725952148 MAE:  0.011187916\n",
      "Train:  15.685677699562458 Vali:  14.17724895477295 MAE:  0.011077149\n",
      "Train:  15.620898794270248 Vali:  14.39827823638916 MAE:  0.010821859\n",
      "Train:  15.55469131984299 Vali:  18.30323600769043 MAE:  0.013679718\n",
      "Train:  15.68179812088287 Vali:  13.5613374710083 MAE:  0.010189558\n",
      "Train:  15.686666453656533 Vali:  14.468064308166504 MAE:  0.011233065\n",
      "Train:  15.56808917505278 Vali:  15.42440414428711 MAE:  0.011867943\n",
      "Train:  15.708998827625521 Vali:  14.748722076416016 MAE:  0.010830129\n",
      "Train:  15.577161760467419 Vali:  14.790534973144531 MAE:  0.010823706\n",
      "Train:  15.653230605365561 Vali:  13.752754211425781 MAE:  0.010983988\n",
      "Train:  15.501259935159478 Vali:  14.508201599121094 MAE:  0.010912714\n",
      "Train:  15.772043527630593 Vali:  15.904582977294922 MAE:  0.012367665\n",
      "Train:  15.593556991412486 Vali:  14.117621421813965 MAE:  0.0103948945\n",
      "Train:  15.591020445171878 Vali:  15.160612106323242 MAE:  0.011587896\n",
      "Train:  15.67053196996236 Vali:  16.812915802001953 MAE:  0.01319238\n",
      "Train:  15.514254959024115 Vali:  15.408819198608398 MAE:  0.011938562\n",
      "Train:  15.558775970747146 Vali:  14.752957344055176 MAE:  0.01111491\n",
      "Train:  15.625761046169472 Vali:  14.162801742553711 MAE:  0.010848196\n",
      "Train:  15.633994394755192 Vali:  14.026426315307617 MAE:  0.010612762\n",
      "Train:  15.653838715107321 Vali:  13.829317092895508 MAE:  0.010780101\n",
      "Train:  15.559511350727767 Vali:  15.824430465698242 MAE:  0.012017345\n",
      "Train:  15.756019998111313 Vali:  13.602763175964355 MAE:  0.010530487\n",
      "Train:  15.527110910758697 Vali:  14.355971336364746 MAE:  0.010429028\n",
      "Train:  15.595206881255555 Vali:  14.55129623413086 MAE:  0.011464575\n",
      "Train:  15.575296312441928 Vali:  14.608243942260742 MAE:  0.010902101\n",
      "Train:  15.6262126600142 Vali:  15.054627418518066 MAE:  0.010826069\n",
      "Train:  15.563886868524895 Vali:  15.114700317382812 MAE:  0.011864737\n",
      "Train:  15.66829637692129 Vali:  13.94931697845459 MAE:  0.011120818\n",
      "Train:  15.570479517188861 Vali:  15.003381729125977 MAE:  0.011475825\n",
      "Train:  15.502537124620067 Vali:  13.838791847229004 MAE:  0.010474908\n",
      "Train:  15.527149507467696 Vali:  14.24853515625 MAE:  0.011161332\n",
      "Train:  15.628083086356842 Vali:  13.053268432617188 MAE:  0.009954675\n",
      "Train:  15.47859179922145 Vali:  14.62795352935791 MAE:  0.011491897\n",
      "Train:  15.570819198141853 Vali:  14.665327072143555 MAE:  0.010736602\n",
      "Train:  15.54280546689205 Vali:  14.665623664855957 MAE:  0.0111411745\n",
      "Train:  15.53514525873198 Vali:  14.326147079467773 MAE:  0.011282348\n",
      "Train:  15.565594200093111 Vali:  15.170018196105957 MAE:  0.011826078\n",
      "Train:  15.55346308886576 Vali:  14.728056907653809 MAE:  0.010744716\n",
      "Train:  15.455036828157713 Vali:  15.856742858886719 MAE:  0.012336577\n",
      "Train:  15.568378967861477 Vali:  14.482951164245605 MAE:  0.0114799645\n",
      "Train:  15.565912483064391 Vali:  14.053862571716309 MAE:  0.0109802885\n",
      "Train:  15.64904021187652 Vali:  13.997291564941406 MAE:  0.01070324\n",
      "Train:  15.529479323188179 Vali:  14.443662643432617 MAE:  0.011346452\n",
      "Train:  15.539829296688382 Vali:  15.182696342468262 MAE:  0.011121378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m x_i, y_i \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     12\u001b[0m     train_X[index[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]], train_Y[index[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]]\n\u001b[0;32m---> 13\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m L \u001b[38;5;241m=\u001b[39m Loss(pred, y_i)\n\u001b[1;32m     15\u001b[0m L\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36mFlexDMPForcing.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/modules/container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/modules/linear.py:93\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abb/lib/python3.9/site-packages/torch/nn/functional.py:1690\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, tens_ops, \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;66;03m# fused op is marginally faster\u001b[39;00m\n\u001b[0;32m-> 1690\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1692\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmatmul(weight\u001b[38;5;241m.\u001b[39mt())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "train_l, vali_l = [], []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    index = np.arange(len(train_X))\n",
    "    np.random.shuffle(index)\n",
    "    L_t = 0.\n",
    "    counter = 0\n",
    "    for i in range(len(train_X)//batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        x_i, y_i = \\\n",
    "            train_X[index[i*batch_size:(i+1)*batch_size]], train_Y[index[i*batch_size:(i+1)*batch_size]]\n",
    "        pred = net(x_i)\n",
    "        L = Loss(pred, y_i)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        L_t += L.item()\n",
    "        counter += 1\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred = net(test_X)\n",
    "        VAE_l = torch.abs(pred - test_Y).sum() / len(pred) * 0.004\n",
    "        L = Loss(pred, test_Y)\n",
    "        vali_l.append(L.item())\n",
    "        train_l.append(L_t/counter)\n",
    "        net.train()\n",
    "    print(\"Train: \", L_t/counter, \"Vali: \", L.item(), \"MAE: \", VAE_l.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abb",
   "language": "python",
   "name": "abb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
